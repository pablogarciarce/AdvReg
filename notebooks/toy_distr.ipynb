{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conjugate_bayes_lin_reg import NormalKnownVariancePriorLinearRegression\n",
    "from src.utils import get_toy_data_indep, plot_ppds\n",
    "from src.attacks.distr_attacks import fgsm_attack, mlmc_attack, mlmc_gradient_estimator, kl_div, kl_to_appd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font=\"serif\")\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1,\n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--',\n",
    "    'legend.fontsize': 12,\n",
    "    'legend.frameon': False,\n",
    "    'figure.dpi': 300,  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_toy_data_indep(100)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gray = X_test[-10:, :]\n",
    "y_gray = y_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for which we know the true gradient\n",
    "model = NormalKnownVariancePriorLinearRegression(prior_params={\n",
    "    'mu': torch.zeros(X_train.shape[1]), \n",
    "    'lam': torch.eye(X_train.shape[1]), \n",
    "    'sigma2': torch.tensor([1])},)\n",
    "data = {'X': torch.tensor(X_train, dtype=torch.float32), 'y': torch.tensor(y_train, dtype=torch.float32)}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_adv = NormalKnownVariancePriorLinearRegression(prior_params={\n",
    "    'mu': torch.zeros(X_train.shape[1]), \n",
    "    'lam': 2 * torch.eye(X_train.shape[1]), \n",
    "    'sigma2': torch.tensor([1])},)\n",
    "data_adv = {'X': torch.tensor(X_gray, dtype=torch.float32), 'y': torch.tensor(y_gray, dtype=torch.float32)}\n",
    "model_adv.fit(data_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPD = N(mu, 2*sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "std = model.sample_predictive_distribution(x, 10000).std()\n",
    "appd = torch.distributions.normal.Normal(x.T @ model.mu, np.sqrt(2) * std)\n",
    "x_adv = x.clone().detach().requires_grad_(True)  \n",
    "optimizer = SGD([x_adv], lr=0.0)\n",
    "x_adv.requires_grad = True\n",
    "optimizer.zero_grad()\n",
    "\n",
    "kl = kl_to_appd(model.mu, model.lam, model.sigma2, x_adv, x.T @ model.mu, 2 * std ** 2)\n",
    "kl.backward()\n",
    "real_grad = x_adv.grad.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = []\n",
    "for it in range(10000): \n",
    "    grads.append(-mlmc_gradient_estimator(appd.sample(), x_adv, 10, model))\n",
    "\n",
    "# Plot 2 histograms, one for each feature\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "for i in range(2):\n",
    "    axs[i].hist([grad[i].item() for grad in grads], bins=20)\n",
    "    axs[i].axvline(real_grad[i].item(), color='red')\n",
    "    axs[i].set_xlabel(f'Gradient {i}')\n",
    "    axs[i].set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack example\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "std = model.sample_predictive_distribution(x, 10000).std()\n",
    "appd = torch.distributions.normal.Normal(x.T @ model.mu, 2 * std)\n",
    "x_adv_distr, x_adv_values = mlmc_attack(model, x, appd=appd, epsilon=2, R=10, lr=.1, n_iter=400)\n",
    "plot_ppds(model, x, x_adv_distr, appd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Punto inicial: {x} con varianza {std**2}.\\n Punto final: {x_adv_distr} con varianza {model.sample_predictive_distribution(x_adv_distr, 10000).var()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector field of perturbations\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "def compute_perturbation(i):\n",
    "    x = torch.tensor(X_test[i,:].copy(), dtype=torch.float32).unsqueeze(1)\n",
    "    std = model.sample_predictive_distribution(x, 10000).std()\n",
    "    appd = torch.distributions.normal.Normal(x.T @ model.mu, 2 * std)\n",
    "    x_adv, x_adv_values = mlmc_attack(model, x, appd, R=10, lr=0.01, n_iter=400)\n",
    "    return x_adv.clone().detach().numpy() - x.clone().detach().numpy()\n",
    "\n",
    "perturbations = Parallel(n_jobs=-1)(delayed(compute_perturbation)(i) for i in range(X_test.shape[0]))\n",
    "\n",
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=50, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) + min(np.array(perturbations)[:,0])-.1,max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1])-.1,max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.2)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "plt.scatter(X_train[:,0], X_train[:,1], marker='x', label='Train points')\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=50, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) + min(np.array(perturbations)[:,0])-.1,max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1])-.1,max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.2)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL divergence as a function of the strength of the attack  \n",
    "n_jobs = -1\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "std = model.sample_predictive_distribution(x, 10000).std()\n",
    "appd = torch.distributions.normal.Normal(x.T @ model.mu, 2 * std)\n",
    "\n",
    "# Function to calculate KL divergence for a given epsilon\n",
    "def compute_kl_divergence(epsilon):\n",
    "    x_adv_distr, _ = mlmc_attack(model, x, appd, epsilon=epsilon, R=10, lr=0.01, n_iter=1000)\n",
    "    kl = kl_to_appd(model.mu, model.lam, model.sigma2, x_adv_distr, x.T @ model.mu, 4 * std ** 2).item()\n",
    "    x_fsgm = fgsm_attack(model, x, appd, epsilon=epsilon, R=10)\n",
    "    kl_fsgm = kl_to_appd(model.mu, model.lam, model.sigma2, x_fsgm, x.T @ model.mu, 4 * std ** 2).item()\n",
    "    x_gray, _ = mlmc_attack(model_adv, x, appd, epsilon=epsilon, R=10, lr=0.01, n_iter=1000)\n",
    "    kl_gray = kl_to_appd(model.mu, model.lam, model.sigma2, x_gray, x.T @ model.mu, 4 * std ** 2).item()\n",
    "    return kl, kl_fsgm, kl_gray\n",
    "\n",
    "\n",
    "# Define the range of epsilon values\n",
    "epsilons = np.linspace(0, 1, 20)\n",
    "kl_alls = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Use joblib to parallelize the computation of KL divergences\n",
    "    kl_values = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_kl_divergence)(epsilon) for epsilon in epsilons  \n",
    "    )\n",
    "    \n",
    "    kl_alls.append(kl_values)\n",
    "\n",
    "kl_mlmc = np.array(kl_alls)[:, :, 0]\n",
    "kl_fsgm = np.array(kl_alls)[:, :, 1]\n",
    "kl_gray = np.array(kl_alls)[:, :, 2]\n",
    "# error bars as std\n",
    "#plt.errorbar(epsilons, np.mean(kl_fsgm, axis=0), yerr=2*np.std(kl_fsgm, axis=0), label='FGSM')\n",
    "plt.errorbar(epsilons, np.mean(kl_gray, axis=0, where=~np.isnan(kl_gray)), yerr=2*np.std(kl_gray, axis=0, where=~np.isnan(kl_gray)), label='MLMC (gray)')\n",
    "plt.errorbar(epsilons, np.mean(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), yerr=2*np.std(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), label='MLMC')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('KL Divergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(epsilons, np.mean(kl_gray, axis=0, where=~np.isnan(kl_gray)), yerr=2*np.std(kl_gray, axis=0, where=~np.isnan(kl_gray)), label='MLMC (gray)')\n",
    "plt.errorbar(epsilons, np.mean(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), yerr=2*np.std(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), label='MLMC')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.ylim(0.5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPD = N(2+mu, 2*sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack example\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "std = model.sample_predictive_distribution(x, 10000).std()\n",
    "appd = torch.distributions.normal.Normal(2 + x.T @ model.mu, 2 * std)\n",
    "x_adv_distr, x_adv_values = mlmc_attack(model, x, appd=appd, epsilon=2, R=10, lr=0.1, n_iter=400)\n",
    "plot_ppds(model, x, x_adv_distr, appd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector field of perturbations\n",
    "def compute_perturbation(i):\n",
    "    x = torch.tensor(X_test[i,:].copy(), dtype=torch.float32).unsqueeze(1)\n",
    "    std = model.sample_predictive_distribution(x, 10000).std()\n",
    "    appd = torch.distributions.normal.Normal(2 + x.T @ model.mu, 2 * std)\n",
    "    x_adv, x_adv_values = mlmc_attack(model, x, appd, R=10, lr=0.01, n_iter=400)\n",
    "    return x_adv.clone().detach().numpy() - x.clone().detach().numpy()\n",
    "\n",
    "perturbations = Parallel(n_jobs=-1)(delayed(compute_perturbation)(i) for i in range(X_test.shape[0]))\n",
    "\n",
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=20, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) - min(np.array(perturbations)[:,0])-.2,max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1])-.3,max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.1)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL divergence as a function of the strength of the attack \n",
    "n_jobs = -1\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "std = model.sample_predictive_distribution(x, 10000).std()\n",
    "appd = torch.distributions.normal.Normal(2 + x.T @ model.mu, 2 * std)\n",
    "\n",
    "# Function to calculate KL divergence for a given epsilon\n",
    "def compute_kl_divergence(epsilon):\n",
    "    x_adv_distr, _ = mlmc_attack(model, x, appd, epsilon=epsilon, R=10, lr=0.01, n_iter=1000)\n",
    "    kl = kl_to_appd(model.mu, model.lam, model.sigma2, x_adv_distr, 2 + x.T @ model.mu, 4 * std ** 2).item()\n",
    "    x_fsgm = fgsm_attack(model, x, appd, epsilon=epsilon, R=10)\n",
    "    kl_fsgm = kl_to_appd(model.mu, model.lam, model.sigma2, x_fsgm, 2 + x.T @ model.mu, 4 * std ** 2).item()\n",
    "    x_gray, _ = mlmc_attack(model_adv, x, appd, epsilon=epsilon, R=10, lr=0.01, n_iter=1000)\n",
    "    kl_gray = kl_to_appd(model.mu, model.lam, model.sigma2, x_gray, 2 + x.T @ model.mu, 4 * std ** 2).item()\n",
    "    return kl, kl_fsgm, kl_gray\n",
    "\n",
    "\n",
    "# Define the range of epsilon values\n",
    "epsilons = np.linspace(0, 1, 20)\n",
    "kl_alls = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Use joblib to parallelize the computation of KL divergences\n",
    "    kl_values = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_kl_divergence)(epsilon) for epsilon in epsilons  \n",
    "    )\n",
    "    #kl_values = [compute_kl_divergence(epsilon) for epsilon in epsilons]\n",
    "    kl_alls.append(kl_values)\n",
    "\n",
    "kl_mlmc = np.array(kl_alls)[:, :, 0]\n",
    "kl_fsgm = np.array(kl_alls)[:, :, 1]\n",
    "kl_gray = np.array(kl_alls)[:, :, 2]\n",
    "# error bars as std\n",
    "#plt.errorbar(epsilons, np.mean(kl_fsgm, axis=0), yerr=2*np.std(kl_fsgm, axis=0), label='FGSM')\n",
    "plt.errorbar(epsilons, np.mean(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), yerr=2*np.std(kl_mlmc, axis=0, where=~np.isnan(kl_mlmc)), label='MLMC')\n",
    "plt.errorbar(epsilons, np.mean(kl_gray, axis=0, where=~np.isnan(kl_gray)), yerr=2*np.std(kl_gray, axis=0, where=~np.isnan(kl_gray)), label='MLMC (gray)')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('KL Divergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum disruption problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "lr = 0.0\n",
    "mu_n = model.mu\n",
    "lam_n = model.lam\n",
    "sigma2 = model.sigma2\n",
    "x_adv = x.clone().detach().requires_grad_(True)  \n",
    "optimizer = SGD([x_adv], lr=0.0)\n",
    "x_adv.requires_grad = True\n",
    "optimizer.zero_grad()\n",
    "\n",
    "kl = - kl_div(mu_n, lam_n, sigma2, x, x_adv)  # maximum disruption problem\n",
    "kl.backward()\n",
    "real_grad = x_adv.grad.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = []\n",
    "for it in range(1000):  # TODO: change to 10000\n",
    "    y = model.sample_predictive_distribution(x, num_samples=1)\n",
    "    grads.append(mlmc_gradient_estimator(y, x_adv, 10, model))\n",
    "\n",
    "# Plot 2 histograms, one for each feature\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "for i in range(2):\n",
    "    axs[i].hist([grad[i].item() for grad in grads], bins=20)\n",
    "    axs[i].axvline(real_grad[i].item(), color='red')\n",
    "    axs[i].set_xlabel(f'Gradient {i}')\n",
    "    axs[i].set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack example\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "x_adv_distr, x_adv_values = mlmc_attack(model, x, epsilon=.5, R=10, lr=0.01, n_iter=400)\n",
    "plot_ppds(model, x, x_adv_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector field of perturbations\n",
    "perturbations = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    x = torch.tensor(X_test[i,:].copy(), dtype=torch.float32).unsqueeze(1)\n",
    "    x_adv, x_adv_values = mlmc_attack(model, x, R=10, lr=0.01)\n",
    "    perturbations.append(x_adv.clone().detach().numpy() - x.clone().detach().numpy())\n",
    "\n",
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=40, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) + min(np.array(perturbations)[:,0]),max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1]),max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.1)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot KL divergence as a function of the strength of the attack \n",
    "n_jobs = 66\n",
    "x = X_test[1,:].copy()\n",
    "x = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Function to calculate KL divergence for a given epsilon\n",
    "def compute_kl_divergence(epsilon):\n",
    "    x_adv_distr, _ = mlmc_attack(model, x, epsilon=epsilon, R=10)\n",
    "    kl = kl_div(model.mu, model.lam, model.sigma2, x, x_adv_distr).item()\n",
    "    x_fsgm = fgsm_attack(model, x, epsilon=epsilon, R=10)\n",
    "    kl_fsgm = kl_div(model.mu, model.lam, model.sigma2, x, x_fsgm).item()\n",
    "    return kl, kl_fsgm\n",
    "\n",
    "\n",
    "# Define the range of epsilon values\n",
    "epsilons = np.linspace(0, 1, 20)\n",
    "kl_alls = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # Use joblib to parallelize the computation of KL divergences\n",
    "    kl_values = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_kl_divergence)(epsilon) for epsilon in epsilons  \n",
    "    )\n",
    "    kl_alls.append(kl_values)\n",
    "\n",
    "kl_mlmc = np.array(kl_alls)[:, :, 0]\n",
    "kl_fsgm = np.array(kl_alls)[:, :, 1]\n",
    "# error bars as std\n",
    "plt.errorbar(epsilons, np.mean(kl_fsgm, axis=0), yerr=2*np.std(kl_fsgm, axis=0), label='FGSM')\n",
    "plt.errorbar(epsilons, np.mean(kl_mlmc, axis=0), yerr=2*np.std(kl_mlmc, axis=0), label='MLMC')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('KL Divergence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advReg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
