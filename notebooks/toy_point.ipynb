{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conjugate_bayes_lin_reg import NormalKnownVariancePriorLinearRegression\n",
    "from src.utils import get_toy_data_indep, id\n",
    "\n",
    "from src.attacks.point_attacks import attack, attack_fgsm, true_gradient_mean, reparametrization_trick, det_attack\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font=\"serif\")\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1,\n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--',\n",
    "    'legend.fontsize': 12,\n",
    "    'legend.frameon': False,\n",
    "    'figure.dpi': 300,  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy data\n",
    "X, y = get_toy_data_indep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n",
    "X_train, X_gray, y_train, y_gray = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = NormalKnownVariancePriorLinearRegression(prior_params={\n",
    "    'mu': torch.zeros(X_train.shape[1]), \n",
    "    'lam': torch.eye(X_train.shape[1]), \n",
    "    'sigma2': torch.tensor([1])},)\n",
    "data = {'X': torch.tensor(X_train, dtype=torch.float32), 'y': torch.tensor(y_train, dtype=torch.float32)}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model_adv = NormalKnownVariancePriorLinearRegression(prior_params={\n",
    "    'mu': torch.zeros(X_train.shape[1]), \n",
    "    'lam': 2 * torch.eye(X_train.shape[1]), \n",
    "    'sigma2': torch.tensor([1])},)\n",
    "data_adv = {'X': torch.tensor(X_gray, dtype=torch.float32), 'y': torch.tensor(y_gray, dtype=torch.float32)}\n",
    "model_adv.fit(data_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_adv = torch.tensor(X_test[17,:], dtype=torch.float32, requires_grad=True)\n",
    "y_star = 3.0\n",
    "grads = []\n",
    "exact_gradient = true_gradient_mean(x_adv, model, y_star)\n",
    "\n",
    "for _ in range(10000):\n",
    "    rep_gradients, _, _ = reparametrization_trick(x_adv, model, y_star, 100, id)\n",
    "    grads.append(rep_gradients.clone().numpy())\n",
    "    x_adv.grad.zero_()\n",
    "    \n",
    "\n",
    "# plot histogram of gradients\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].hist(np.array(grads)[:, 0], bins=20)\n",
    "# plot line at exact_gradient[0]\n",
    "axs[0].axvline(exact_gradient[0].item(), color='r')\n",
    "\n",
    "axs[1].hist(np.array(grads)[:, 1], bins=20)\n",
    "# plot line at exact_gradient[1]\n",
    "axs[1].axvline(exact_gradient[1].item(), color='r')\n",
    "\n",
    "# label the axes\n",
    "axs[0].set_xlabel('Gradient 0')\n",
    "axs[0].set_ylabel('Counts')\n",
    "axs[1].set_xlabel('Gradient 1')\n",
    "axs[1].set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector field of perturbations for toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector field of perturbations\n",
    "y_star = 3.0\n",
    "epsilon = 0.1\n",
    "\n",
    "perturbations = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    x_adv = torch.tensor(X_test[i,:].copy(), dtype=torch.float32, requires_grad=True)\n",
    "    x_adv_values, loss_values, func_values = attack(x_adv, model, y_star, epsilon=epsilon, samples_per_iteration=100, learning_rate=1e-4, num_iterations=1000, func=id)\n",
    "    perturbations.append(x_adv_values[-1] - x_adv.clone().detach().numpy())\n",
    "\n",
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=40, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) + min(np.array(perturbations)[:,0])-.2,max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1]),max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.2)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perturbations for deterministic attack as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector field of perturbations FOR DETERMINISTIC ATTACK\n",
    "y_star = 3.0\n",
    "epsilon = 0.1\n",
    "\n",
    "perturbations = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    x_adv = torch.tensor(X_test[i,:].copy(), dtype=torch.float32)\n",
    "    x_adv_det, _ = det_attack(x_adv, model, y_star, epsilon=epsilon)\n",
    "    perturbations.append(x_adv_det.numpy() - x_adv.clone().detach().numpy())\n",
    "\n",
    "plt.quiver(X_test[:,0], X_test[:,1], np.array(perturbations)[:,0], np.array(perturbations)[:,1], \n",
    "           width=0.002, angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# add beta direction\n",
    "mu = model.mu.numpy()\n",
    "plt.quiver(0, 0, mu[0], mu[1], color='b',\n",
    "           width=0.005, angles='xy', scale_units='xy', scale=40, label='Coefficients direction')\n",
    "plt.xlim(min(X_test[:,0]) + min(np.array(perturbations)[:,0])-.2,max(X_test[:,0]) + max(np.array(perturbations)[:,0]))\n",
    "plt.ylim(min(X_test[:,1]) + min(np.array(perturbations)[:,1]),max(X_test[:,1]) + max(np.array(perturbations)[:,1]) + 0.2)  # room for legend\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security evaluation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss as a function of the size of the perturbation for both deterministic and reparametrization trick attacks\n",
    "y_star = 3\n",
    "\n",
    "losses_det_all = []\n",
    "losses_rep_all = []\n",
    "losses_fgsm_all = []\n",
    "losses_gray_all = []\n",
    "epsilons = np.linspace(0, .5, 10)\n",
    "x_adv = torch.tensor(X_test[17,:].copy(), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "for _ in range(10):\n",
    "    losses_det = []\n",
    "    losses_rep = []\n",
    "    losses_fgsm = []\n",
    "    losses_gray = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        x_adv_det, y_adv_det = det_attack(X_test[17,:].copy(), model, y_star, epsilon, verbose=False)\n",
    "        losses_det.append((y_adv_det - y_star) ** 2)\n",
    "        if epsilon > .3:\n",
    "            lr = 1e-5\n",
    "        else: \n",
    "            lr = 1e-4\n",
    "        x_adv_values, loss_values, func_values = attack(x_adv, model, y_star, epsilon=epsilon, learning_rate=lr, func=id, samples_per_iteration=1000, num_iterations=2000)   \n",
    "        y_adv = model.sample_predictive_distribution(torch.tensor(x_adv_values[-1]).unsqueeze(1), 1000).mean().item() \n",
    "        losses_rep.append((y_adv - y_star) ** 2)\n",
    "        \n",
    "        x_gray_values, loss_values, func_values = attack(x_adv, model_adv, y_star, epsilon=epsilon, learning_rate=lr, func=id, samples_per_iteration=1000, num_iterations=2000)\n",
    "        y_adv_gray = model.sample_predictive_distribution(torch.tensor(x_gray_values[-1]).unsqueeze(1), 1000).mean().item()\n",
    "        losses_gray.append((y_adv_gray - y_star) ** 2)\n",
    "\n",
    "        x_adv_fgsm = attack_fgsm(torch.tensor(X_test[17,:].copy(), dtype=torch.float32), model, y_star, epsilon=epsilon)\n",
    "        y_adv_fgsm = model.sample_predictive_distribution(x_adv_fgsm.unsqueeze(1), 1000).mean().item()\n",
    "        losses_fgsm.append((y_adv_fgsm - y_star) ** 2)\n",
    "\n",
    "    losses_det_all.append(losses_det)\n",
    "    losses_rep_all.append(losses_rep)\n",
    "    losses_fgsm_all.append(losses_fgsm)\n",
    "    losses_gray_all.append(losses_gray)\n",
    "\n",
    "# plot the mean and std of the losses\n",
    "plt.errorbar(epsilons, np.mean(losses_fgsm_all, axis=0), yerr=2*np.std(losses_fgsm_all, axis=0), label='FGSM')\n",
    "plt.errorbar(epsilons, np.mean(losses_rep_all, axis=0), yerr=2*np.std(losses_rep_all, axis=0), label='Reparametrization trick')\n",
    "plt.errorbar(epsilons, np.mean(losses_det_all, axis=0), yerr=2*np.std(losses_det_all, axis=0), label='Deterministic')\n",
    "plt.errorbar(epsilons, np.mean(losses_gray_all, axis=0), yerr=2*np.std(losses_gray_all, axis=0), label='Gray-box')\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum disruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss as a function of the size of the perturbation for both deterministic and reparametrization trick attacks\n",
    "y_star = None\n",
    "\n",
    "losses_rep_all = []\n",
    "losses_fgsm_all = []\n",
    "epsilons = np.linspace(0, .5, 10)\n",
    "x_adv = torch.tensor(X_test[17,:].copy(), dtype=torch.float32, requires_grad=True)\n",
    "y_0 = model.sample_predictive_distribution(torch.tensor(X_test[17,:], dtype=torch.float32).unsqueeze(1), 1000).mean().item()\n",
    "\n",
    "for _ in range(10):\n",
    "    losses_rep = []\n",
    "    losses_fgsm = []\n",
    "    losses_gray = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        if epsilon > .3:\n",
    "            lr = 1e-5\n",
    "        else: \n",
    "            lr = 1e-4\n",
    "        x_adv_values, loss_values, func_values = attack(x_adv, model, y_star, epsilon=epsilon, learning_rate=lr, func=id, samples_per_iteration=1000, num_iterations=2000)   \n",
    "        y_adv = model.sample_predictive_distribution(torch.tensor(x_adv_values[-1]).unsqueeze(1), 1000).mean().item() \n",
    "        losses_rep.append((y_adv - y_0) ** 2)\n",
    "\n",
    "        x_adv_fgsm = attack_fgsm(torch.tensor(X_test[17,:].copy(), dtype=torch.float32), model, y_star, epsilon=epsilon)\n",
    "        y_adv_fgsm = model.sample_predictive_distribution(x_adv_fgsm.unsqueeze(1), 1000).mean().item()\n",
    "        losses_fgsm.append((y_adv_fgsm - y_0) ** 2)\n",
    "\n",
    "    losses_fgsm_all.append(losses_fgsm)\n",
    "    losses_rep_all.append(losses_rep)\n",
    "\n",
    "# plot the mean and std of the losses\n",
    "plt.errorbar(epsilons, np.mean(losses_fgsm_all, axis=0), yerr=2*np.std(losses_fgsm_all, axis=0), label='FGSM')\n",
    "plt.errorbar(epsilons, np.mean(losses_rep_all, axis=0), yerr=2*np.std(losses_rep_all, axis=0), label='Reparametrization trick')\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advReg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
