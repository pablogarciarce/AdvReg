{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conjugate_bayes_lin_reg import NormalInverseGammaPriorLinearRegression\n",
    "\n",
    "from src.attacks.distr_attacks import mlmc_attack, kl_div, kl_to_appd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font=\"serif\")\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1,\n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--',\n",
    "    'legend.fontsize': 12,\n",
    "    'legend.frameon': False,\n",
    "    'figure.dpi': 300,  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of 3 datasets: Wine, energy and housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "data = pd.read_excel(url)\n",
    "\n",
    "# The last two columns are the target variables (Heating Load and Cooling Load)\n",
    "X = data.iloc[:, :-2].values  # Covariates\n",
    "y = data.iloc[:, -2].values  # Heating Load\n",
    "y_energy = y / 4  # to make the target variable similar to the other datasets\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_energy = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality dataset with response in {3, 4, 5, 6, 7, 8}, 11 features and 4898 samples\n",
    "\n",
    "# URL to the Wine Quality dataset (for example, from UCI Machine Learning Repository)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "# Load the dataset directly into a Pandas DataFrame\n",
    "data = pd.read_csv(url, delimiter=\";\")\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y_wine = data.iloc[:, -1].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_wine = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing dataset\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = california_housing.data.values\n",
    "y_housing = california_housing.target.values\n",
    "\n",
    "X_housing = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all 3 datasets: Fit the model, compute the average rmse over the test set for the original data and the adversarial examples with epsilon = 0.2 and epsilon = 0.5\n",
    "datasets = ['energy', 'housing', 'wine']\n",
    "epsilons = [0, 0.2, 0.5]\n",
    "\n",
    "results = []\n",
    "results_attack = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f'Running dataset {dataset}')\n",
    "    if dataset == 'energy':\n",
    "        X, y = X_energy, y_energy\n",
    "    elif dataset == 'wine':\n",
    "        X, y = X_wine, y_wine\n",
    "    elif dataset == 'housing':\n",
    "        X, y = X_housing, y_housing\n",
    "\n",
    "    results_dataset = []\n",
    "    results_dataset_attack = []\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "\n",
    "        model = NormalInverseGammaPriorLinearRegression(prior_params={\n",
    "            'mu': torch.zeros(X_train.shape[1]), \n",
    "            'lam': torch.eye(X_train.shape[1]), \n",
    "            'a': torch.tensor([10]), \n",
    "            'b': torch.tensor([1])}\n",
    "            )\n",
    "        data = {'X': torch.tensor(X_train, dtype=torch.float32), 'y': torch.tensor(y_train, dtype=torch.float32)}\n",
    "        model.fit(data)\n",
    "\n",
    "        X_test = X_test[:88, :]  # 88 is the njobs number, so we can parallelize the computation efficiently\n",
    "        y_test = y_test[:88]\n",
    "\n",
    "        res_it = []\n",
    "        res_it_attack = []\n",
    "        for epsilon in epsilons:\n",
    "            def compute_kl(i):\n",
    "                x = torch.tensor(X_test[i,:].copy(), dtype=torch.float32).unsqueeze(1)\n",
    "                std = model.sample_predictive_distribution(x, 1000).std()\n",
    "                appd = torch.distributions.normal.Normal(2 + x.T @ model.mu, 2 * std)\n",
    "                if epsilon == 0:\n",
    "                    x_adv = x.clone().detach()\n",
    "                else:\n",
    "                    x_adv, _ = mlmc_attack(model, x, appd, epsilon=epsilon, verbose=False, R=10, lr=0.01, n_iter=800)\n",
    "                sigma2 = model.sample_posterior_distribution(1000)[1].mean()\n",
    "                att_kl = kl_to_appd(model.mu, model.lam, sigma2, x_adv, 2 * x.T @ model.mu, 4 * std ** 2).item()\n",
    "                df_kl = kl_div(model.mu, model.lam, sigma2, x, x_adv).item()\n",
    "                return att_kl#, df_kl\n",
    "\n",
    "            kl_values = Parallel(n_jobs=-1)(delayed(compute_kl)(i) for i in range(X_test.shape[0]))\n",
    "            #kl_values_attack = np.array(kl_values)[]\n",
    "            kl = sum(kl_values)\n",
    "            res_it.append(kl / X_test.shape[0])\n",
    "        results_dataset.append(res_it)\n",
    "    results.append(results_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as a table\n",
    "results_array = np.array(results).mean(axis=1)\n",
    "results_array = np.round(results_array, 3)\n",
    "results_df = pd.DataFrame(results_array, columns=epsilons, index=datasets)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as a table\n",
    "results_array = 2 * np.array(results).std(axis=1)\n",
    "results_array = np.round(results_array, 3)\n",
    "results_df = pd.DataFrame(results_array, columns=epsilons, index=datasets)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advReg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
