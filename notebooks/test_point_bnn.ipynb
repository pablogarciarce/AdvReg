{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.conjugate_bayes_lin_reg import NormalInverseGammaPriorLinearRegression\n",
    "from src.models.bayesian_nn import BayesianNN\n",
    "from src.utils import id, expy2, l1_projection, plot_ppds\n",
    "\n",
    "from src.attacks.point_attacks import attack, attack_fgsm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font=\"serif\")\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.linewidth': 1,\n",
    "    'grid.alpha': 0.5,\n",
    "    'grid.linestyle': '--',\n",
    "    'legend.fontsize': 12,\n",
    "    'legend.frameon': False,\n",
    "    'figure.dpi': 300,  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine Quality dataset with response in {3, 4, 5, 6, 7, 8}, 11 features and 4898 samples\n",
    "\n",
    "# URL to the Wine Quality dataset (for example, from UCI Machine Learning Repository)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "# Load the dataset directly into a Pandas DataFrame\n",
    "data = pd.read_csv(url, delimiter=\";\")\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "numpyro.set_host_device_count(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model = BayesianNN(input_dim=X_train.shape[1], hidden_units=3)\n",
    "try:\n",
    "    model.load('../src/models/bayesian_3nn')\n",
    "except FileNotFoundError:   \n",
    "    print('Not model found, fitting the model') \n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        num_warmup=500,\n",
    "        num_samples=1000,\n",
    "        num_chains=88,\n",
    "    )\n",
    "    model.save('../src/models/bayesian_3nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample_predictive_distribution(\n",
    "    torch.tensor(X_test[17,:].copy(), dtype=torch.float32).unsqueeze(1), \n",
    "    1000\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack with mean as objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss as a function of the size of the perturbation for both deterministic and reparametrization trick attacks\n",
    "losses_rep_all = []\n",
    "losses_fgsm_all = []\n",
    "epsilons = np.linspace(0, .5, 10)\n",
    "y_star = 9.0\n",
    "\n",
    "for _ in range(4):\n",
    "    losses_rep = []\n",
    "    losses_fgsm = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        x_adv = torch.tensor(X_test[17,:].copy(), dtype=torch.float32)\n",
    "        x_adv_values, loss_values, func_values = attack(x_adv, model, y_star, epsilon=epsilon, learning_rate=1e-4, func=id)  \n",
    "        y_adv = model.sample_predictive_distribution(torch.tensor(x_adv_values[-1], dtype=torch.float32).unsqueeze(1), 1000).mean()\n",
    "        losses_rep.append((y_adv - y_star) ** 2)\n",
    "\n",
    "        x_adv_fgsm = attack_fgsm(torch.tensor(X_test[17,:].copy(), dtype=torch.float32), model, y_star, epsilon=epsilon)\n",
    "        y_adv_fgsm = model.sample_predictive_distribution(x_adv_fgsm.unsqueeze(1), 1000).mean()\n",
    "        losses_fgsm.append((y_adv_fgsm - y_star) ** 2)\n",
    "\n",
    "    losses_rep_all.append(losses_rep)\n",
    "    losses_fgsm_all.append(losses_fgsm)\n",
    "\n",
    "# plot the mean and std of the losses\n",
    "plt.errorbar(epsilons, np.mean(losses_rep_all, axis=0), yerr=2*np.std(losses_rep_all, axis=0), label='Reparametrization trick')\n",
    "plt.errorbar(epsilons, np.mean(losses_fgsm_all, axis=0), yerr=2*np.std(losses_fgsm_all, axis=0), label='FGSM')\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack with exp(y^2/100) as objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = model.sample_predictive_distribution(torch.tensor(X_test[17,:], dtype=torch.float32).unsqueeze(1), 1000)\n",
    "func = torch.exp(ys ** 2 / 100)\n",
    "func.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss as a function of the size of the perturbation for reparametrization trick attack\n",
    "x_adv = torch.tensor(X_test[17,:].copy(), dtype=torch.float32, requires_grad=True)\n",
    "y_star = 3.0\n",
    "losses_total_rep = []\n",
    "losses_total_fgsm = []\n",
    "epsilons = np.linspace(0, .8, 10)\n",
    "\n",
    "def compute_loss_rep(epsilon):\n",
    "    x_adv_values, loss_values, func_values = attack(x_adv, model, y_star, epsilon=epsilon, learning_rate=1e-4, num_iterations=1000, samples_per_iteration=1000, func=expy2)    \n",
    "    y_adv = model.sample_predictive_distribution(torch.tensor(x_adv_values[-1]).unsqueeze(1), 1000)\n",
    "    func_mean = torch.exp(y_adv ** 2 / 100).mean()\n",
    "    return (func_mean - y_star) ** 2\n",
    "\n",
    "def compute_loss_fgsm(epsilon):\n",
    "    x_adv_fgsm = attack_fgsm(x_adv, model, y_star, epsilon=epsilon, func=expy2)\n",
    "    y_adv_fgsm = model.sample_predictive_distribution(x_adv_fgsm.unsqueeze(1), 1000)\n",
    "    func_mean = torch.exp(y_adv_fgsm ** 2 / 100).mean()\n",
    "    return (func_mean - y_star) ** 2\n",
    "\n",
    "for _ in range(4):\n",
    "    print(_)\n",
    "    losses_rep = []\n",
    "    losses_fgsm = []\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        losses_rep.append(compute_loss_rep(epsilon))\n",
    "        losses_fgsm.append(compute_loss_fgsm(epsilon))\n",
    "        \n",
    "    losses_total_rep.append(losses_rep)\n",
    "    losses_total_fgsm.append(losses_fgsm)\n",
    "\n",
    "# plot the mean and std of the losses\n",
    "plt.errorbar(epsilons, np.mean(losses_total_rep, axis=0), yerr=2*np.std(losses_total_rep, axis=0), label='Reparametrization trick')\n",
    "plt.errorbar(epsilons, np.mean(losses_total_fgsm, axis=0), yerr=2*np.std(losses_total_fgsm, axis=0), label='FGSM')\n",
    "plt.legend()\n",
    "plt.xlabel('$\\epsilon$')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare attacking one point with l1 or l2 projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X_test[17,:], dtype=torch.float32)\n",
    "y_star = 3.0\n",
    "x_adv_values, loss_values, func_values = attack(x, model, y_star, epsilon=0.3, learning_rate=1e-4, num_iterations=1000, samples_per_iteration=1000, func=expy2, projection=l1_projection)\n",
    "x_adv_l1 = x_adv_values[-1]\n",
    "x_adv_values, loss_values, func_values = attack(x, model, y_star, epsilon=0.3, learning_rate=1e-4, num_iterations=1000, samples_per_iteration=1000, func=expy2)\n",
    "x_adv_l2 = x_adv_values[-1]\n",
    "print(x,'\\n', x_adv_l1,'\\n', x_adv_l2)\n",
    "print(x-x_adv_l1, '\\n', x-x_adv_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advReg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
